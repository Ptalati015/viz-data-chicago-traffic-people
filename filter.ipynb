{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10744\\3990411327.py:12: DtypeWarning: Columns (20,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traffic = pd.read_csv(csv_file)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Replace 'Traffic_Crashes_-_People.csv' with your CSV file path\n",
    "csv_file = 'Traffic_Crashes_-_People.csv'\n",
    "\n",
    "# Replace 'output.json' with the desired JSON file path\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "traffic = pd.read_csv(csv_file)\n",
    "\n",
    "# Filter the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block is for the linked visualization 1\n",
    "json_file = 'Deliverable3_linked1.json'\n",
    "\n",
    "traffic['YEAR'] = pd.to_datetime(traffic['CRASH_DATE']).dt.year\n",
    "filtered_data = traffic[traffic['INJURY_CLASSIFICATION'] != 'NO INDICATION OF INJURY']\n",
    "top_cities = filtered_data['CITY'].value_counts().head(5).index\n",
    "top_cities = top_cities[top_cities != 'CHICAGO']  # Exclude Chicago\n",
    "\n",
    "# Create a list of dictionaries in the desired format\n",
    "json_data = []\n",
    "\n",
    "for year in filtered_data['YEAR'].unique():\n",
    "    for city in top_cities:\n",
    "        for injury_classification in filtered_data['INJURY_CLASSIFICATION'].unique():\n",
    "            count = len(filtered_data[(filtered_data['YEAR'] == year) &\n",
    "                                      (filtered_data['CITY'] == city) &\n",
    "                                      (filtered_data['INJURY_CLASSIFICATION'] == injury_classification)])\n",
    "            if count > 0:\n",
    "                data_point = {\n",
    "                    'Year': int(year),  # Convert to native Python int\n",
    "                    'City': city,\n",
    "                    'Injury Classification': injury_classification,\n",
    "                    'Count': count\n",
    "                }\n",
    "                json_data.append(data_point)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(json_file, 'w') as json_output:\n",
    "    json.dump(json_data, json_output, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_26168\\3354939895.py:11: DtypeWarning: Columns (20,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traffic = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m json_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutput.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m traffic \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(csv_file)\n\u001b[0;32m     12\u001b[0m traffic_1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mTraffic_Crashes_-_Crashes_20231011.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m safety_equipment_values \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mHELMET USED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNONE PRESENT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSAFETY BELT USED\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1795\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1792\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1793\u001b[0m         new_rows \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(index)\n\u001b[1;32m-> 1795\u001b[0m     df \u001b[39m=\u001b[39m DataFrame(col_dict, columns\u001b[39m=\u001b[39mcolumns, index\u001b[39m=\u001b[39mindex)\n\u001b[0;32m   1797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_rows\n\u001b[0;32m   1799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqueeze \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(df\u001b[39m.\u001b[39mcolumns) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, typ\u001b[39m=\u001b[39mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39mdtype, typ\u001b[39m=\u001b[39mtyp, consolidate\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:154\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    151\u001b[0m axes \u001b[39m=\u001b[39m [columns, index]\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m create_block_manager_from_column_arrays(\n\u001b[0;32m    155\u001b[0m         arrays, axes, consolidate\u001b[39m=\u001b[39mconsolidate\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    157\u001b[0m \u001b[39melif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2204\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate)\u001b[0m\n\u001b[0;32m   2202\u001b[0m     \u001b[39mraise\u001b[39;00m construction_error(\u001b[39mlen\u001b[39m(arrays), arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, axes, e)\n\u001b[0;32m   2203\u001b[0m \u001b[39mif\u001b[39;00m consolidate:\n\u001b[1;32m-> 2204\u001b[0m     mgr\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m   2205\u001b[0m \u001b[39mreturn\u001b[39;00m mgr\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1871\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_consolidated():\n\u001b[0;32m   1870\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m _consolidate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks)\n\u001b[0;32m   1872\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1873\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs \u001b[39m=\u001b[39m _consolidate_with_refs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2329\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2327\u001b[0m new_blocks: \u001b[39mlist\u001b[39m[Block] \u001b[39m=\u001b[39m []\n\u001b[0;32m   2328\u001b[0m \u001b[39mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[39min\u001b[39;00m grouper:\n\u001b[1;32m-> 2329\u001b[0m     merged_blocks, _ \u001b[39m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2330\u001b[0m         \u001b[39mlist\u001b[39m(group_blocks), dtype\u001b[39m=\u001b[39mdtype, can_consolidate\u001b[39m=\u001b[39m_can_consolidate\n\u001b[0;32m   2331\u001b[0m     )\n\u001b[0;32m   2332\u001b[0m     new_blocks \u001b[39m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2333\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2392\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2389\u001b[0m     new_mgr_locs \u001b[39m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2391\u001b[0m     bp \u001b[39m=\u001b[39m BlockPlacement(new_mgr_locs)\n\u001b[1;32m-> 2392\u001b[0m     \u001b[39mreturn\u001b[39;00m [new_block_2d(new_values, placement\u001b[39m=\u001b[39mbp)], \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m \u001b[39m# can't consolidate --> no merge\u001b[39;00m\n\u001b[0;32m   2395\u001b[0m \u001b[39mreturn\u001b[39;00m blocks, \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:2161\u001b[0m, in \u001b[0;36mnew_block_2d\u001b[1;34m(values, placement)\u001b[0m\n\u001b[0;32m   2157\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m ObjectBlock\n\u001b[0;32m   2158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n\u001b[1;32m-> 2161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_block_2d\u001b[39m(values: ArrayLike, placement: BlockPlacement):\n\u001b[0;32m   2162\u001b[0m     \u001b[39m# new_block specialized to case with\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m     \u001b[39m#  ndim=2\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m     \u001b[39m#  isinstance(placement, BlockPlacement)\u001b[39;00m\n\u001b[0;32m   2165\u001b[0m     \u001b[39m#  check_ndim/ensure_block_shape already checked\u001b[39;00m\n\u001b[0;32m   2166\u001b[0m     klass \u001b[39m=\u001b[39m get_block_type(values\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2168\u001b[0m     values \u001b[39m=\u001b[39m maybe_coerce_values(values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this code block is for the linked visualization 2-1\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Replace 'your_input.csv' with the actual CSV file path\n",
    "csv_file = 'Traffic_Crashes_-_People.csv'\n",
    "\n",
    "# Replace 'output.json' with the desired JSON file path\n",
    "json_file = 'Deliverable3_linked2-1.json'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "traffic = pd.read_csv(csv_file)\n",
    "traffic_1 = pd.read_csv(\"Traffic_Crashes_-_Crashes_20231011.csv\")\n",
    "\n",
    "\n",
    "safety_equipment_values = [\"HELMET USED\", \"NONE PRESENT\", \"SAFETY BELT USED\"]\n",
    "selected_weather_conditions = ['BLOWING SNOW', 'CLOUDY/OVERCAST', 'FOG/SMOKE/HAZE', 'FREEZING RAIN/DRIZZLE', 'RAIN', 'SLEET/HAIL', 'SNOW']\n",
    "\n",
    "filtered_traffic = traffic[traffic['SAFETY_EQUIPMENT'].isin(safety_equipment_values)]\n",
    "\n",
    "# Step 1: Filter out 'clear' weather condition\n",
    "filtered_traffic = traffic_1[traffic_1['WEATHER_CONDITION'] != 'CLEAR']\n",
    "\n",
    "# Step 2: Merge the two dataframes on 'CRASH_RECORD_ID'\n",
    "traffic['WEATHER_CONDITION'] = traffic['CRASH_RECORD_ID'].map(filtered_traffic.set_index('CRASH_RECORD_ID')['WEATHER_CONDITION'])\n",
    "\n",
    "\n",
    "traffic = traffic[traffic['SAFETY_EQUIPMENT'].isin(safety_equipment_values)]\n",
    "traffic = traffic[traffic['WEATHER_CONDITION'].isin(selected_weather_conditions)]\n",
    "# Create a list of dictionaries in the desired format\n",
    "json_data = []\n",
    "\n",
    "# Get unique combinations of SAFETY_EQUIPMENT and WEATHER_CONDITION\n",
    "combinations = traffic[['SAFETY_EQUIPMENT', 'WEATHER_CONDITION']].drop_duplicates()\n",
    "\n",
    "# Iterate through the combinations and count occurrences\n",
    "for index, row in combinations.iterrows():\n",
    "    safety_equipment = row['SAFETY_EQUIPMENT']\n",
    "    weather_condition = row['WEATHER_CONDITION']\n",
    "    count = len(traffic[(traffic['SAFETY_EQUIPMENT'] == safety_equipment) & (traffic['WEATHER_CONDITION'] == weather_condition)])\n",
    "   \n",
    "    data_point = {\n",
    "        'safety equipment': safety_equipment,\n",
    "        'weather condition': weather_condition,\n",
    "        'count': count\n",
    "    }\n",
    "    json_data.append(data_point)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(json_file, 'w') as json_output:\n",
    "    json.dump(json_data, json_output, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_14756\\786814328.py:11: DtypeWarning: Columns (20,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traffic = pd.read_csv(csv_file)\n"
     ]
    }
   ],
   "source": [
    "# this code block is for the linked visualization 2-2\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Replace 'your_input.csv' with the actual CSV file path\n",
    "csv_file = 'Traffic_Crashes_-_People.csv'\n",
    "\n",
    "# Replace 'output.json' with the desired JSON file path\n",
    "json_file = 'Deliverable3_linked2-2.json'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "traffic = pd.read_csv(csv_file)\n",
    "traffic_1 = pd.read_csv(\"Traffic_Crashes_-_Crashes_20231011.csv\")\n",
    "safety_equipment_values = [\"HELMET USED\", \"NONE PRESENT\", \"SAFETY BELT USED\"]\n",
    "selected_weather_conditions = ['BLOWING SNOW', 'CLOUDY/OVERCAST', 'FOG/SMOKE/HAZE', 'FREEZING RAIN/DRIZZLE', 'RAIN', 'SLEET/HAIL', 'SNOW']\n",
    "\n",
    "filtered_traffic = traffic[traffic['SAFETY_EQUIPMENT'].isin(safety_equipment_values)]\n",
    "\n",
    "# Step 1: Filter out 'clear' weather condition\n",
    "filtered_traffic = traffic_1[traffic_1['WEATHER_CONDITION'] != 'CLEAR']\n",
    "\n",
    "# Step 2: Merge the two dataframes on 'CRASH_RECORD_ID'\n",
    "traffic['WEATHER_CONDITION'] = traffic['CRASH_RECORD_ID'].map(filtered_traffic.set_index('CRASH_RECORD_ID')['WEATHER_CONDITION'])\n",
    "traffic = traffic[traffic['SAFETY_EQUIPMENT'].isin(safety_equipment_values)]\n",
    "traffic = traffic[traffic['WEATHER_CONDITION'].isin(selected_weather_conditions)]\n",
    "\n",
    "# Add the year information to the DataFrame\n",
    "traffic['YEAR'] = pd.to_datetime(traffic['CRASH_DATE']).dt.year\n",
    "\n",
    "# Create a list of dictionaries in the desired format\n",
    "json_data = []\n",
    "\n",
    "# Get unique combinations of SAFETY_EQUIPMENT, WEATHER_CONDITION, and YEAR\n",
    "combinations = traffic[['SAFETY_EQUIPMENT', 'WEATHER_CONDITION', 'YEAR']].drop_duplicates()\n",
    "\n",
    "# Iterate through the combinations and count occurrences\n",
    "for index, row in combinations.iterrows():\n",
    "    safety_equipment = row['SAFETY_EQUIPMENT']\n",
    "    weather_condition = row['WEATHER_CONDITION']\n",
    "    year = row['YEAR']\n",
    "    count = len(traffic[(traffic['SAFETY_EQUIPMENT'] == safety_equipment) & (traffic['WEATHER_CONDITION'] == weather_condition) & (traffic['YEAR'] == year)])\n",
    "   \n",
    "    data_point = {\n",
    "        'safety equipment': safety_equipment,\n",
    "        'weather condition': weather_condition,\n",
    "        'count': count,\n",
    "        'year': int(year)  # Include the year component\n",
    "    }\n",
    "    json_data.append(data_point)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(json_file, 'w') as json_output:\n",
    "    json.dump(json_data, json_output, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code block is for the linked visualization 3\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define age groups\n",
    "age_groups = ['0-9 years', '10-19 years', '20-29 years', '30-39 years', '40-49 years',\n",
    "              '50-59 years', '60-69 years', '70+ years']\n",
    "\n",
    "# Initialize counts for each age group\n",
    "male_counts = [0] * len(age_groups)\n",
    "female_counts = [0] * len(age_groups)\n",
    "# traffic['YEAR'] = pd.to_datetime(traffic['CRASH_DATE']).dt.year\n",
    "\n",
    "# Process the data and count males and females in each age group\n",
    "for index, row in traffic.iterrows():\n",
    "    age = row['AGE']\n",
    "    sex = row['SEX']\n",
    "\n",
    "    if not pd.isna(age) and sex in ['M', 'F']:\n",
    "        age = int(age)\n",
    "\n",
    "        # Check if age is within the range of age_groups\n",
    "        if 0 <= age < 80:\n",
    "            age_group_index = age // 10  # Assign to the corresponding age group\n",
    "            if sex == 'M':\n",
    "                male_counts[age_group_index] += 1\n",
    "            elif sex == 'F':\n",
    "                female_counts[age_group_index] += 1\n",
    "\n",
    "# Group data by year\n",
    "json_data  = []\n",
    "\n",
    "for i in range(len(age_groups)):\n",
    "    data_point = {\n",
    "        'bin': age_groups[i],\n",
    "        'male_count': male_counts[i],\n",
    "        'female_count': female_counts[i], \n",
    "        'count': male_counts[i] + female_counts[i]  # Include the year component\n",
    "    }\n",
    "    json_data.append(data_point)\n",
    "    \n",
    "  \n",
    "\n",
    "# Write the JSON data to a file\n",
    "json_file = 'Deliverable3_linked3.json'  # Replace with the desired JSON file path\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(json_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_35432\\3857189036.py:5: DtypeWarning: Columns (20,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traffic = pd.read_csv(\"Traffic_Crashes_-_People.csv\")\n"
     ]
    }
   ],
   "source": [
    "# this code block is for the linked visualization 4\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV data\n",
    "traffic = pd.read_csv(\"Traffic_Crashes_-_People.csv\")\n",
    "\n",
    "# Define a mapping of physical conditions to categories\n",
    "condition_mapping = {\n",
    "    \"IMPAIRED - DRUGS\": \"Poor\",\n",
    "    \"IMPAIRED - ALCOHOL\": \"Poor\",\n",
    "    \"IMPAIRED - ALCOHOL AND DRUGS\": \"Poor\",\n",
    "    \"ILLNESS/FAINTED\": \"Fair\",\n",
    "    \"MEDICATED\": \"Fair\",\n",
    "    \"other\": \"Good\"\n",
    "}\n",
    "\n",
    "# Create a dictionary to store counts for each category\n",
    "category_counts = {\"Good\": 0, \"Fair\": 0, \"Poor\": 0}\n",
    "\n",
    "# Create a list to store the aggregated data\n",
    "result_data = []\n",
    "\n",
    "# Iterate over the rows in the CSV data\n",
    "for index, row in traffic.iterrows():\n",
    "    age = row[\"AGE\"]\n",
    "    physical_condition = row[\"PHYSICAL_CONDITION\"]\n",
    "    \n",
    "    # Check if AGE is not NaN\n",
    "    if not pd.isna(age):\n",
    "        # Determine the category based on the mapping\n",
    "        if physical_condition in condition_mapping:\n",
    "            category = condition_mapping[physical_condition]\n",
    "            # Check if the category count is less than 100\n",
    "            if category_counts[category] < 100:\n",
    "                # Check if the category already exists in the result_data\n",
    "                existing_entry = next((entry for entry in result_data if entry[\"AGE\"] == age and entry[\"PHYSICAL_CONDITION\"] == category), None)\n",
    "\n",
    "                if existing_entry:\n",
    "                    # If the entry already exists, update the count\n",
    "                    existing_entry[\"count\"] += 1\n",
    "                else:\n",
    "                    # If the entry doesn't exist, create a new entry\n",
    "                    result_data.append({\"AGE\": age, \"PHYSICAL_CONDITION\": category, \"count\": 1})\n",
    "                # Update the category count\n",
    "                category_counts[category] += 1\n",
    "\n",
    "# Convert the result_data to a JSON format\n",
    "result_json = json.dumps(result_data, indent=4)\n",
    "\n",
    "# Write the JSON to a file\n",
    "with open(\"output.json\", \"w\") as json_file:\n",
    "    json_file.write(result_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_36288\\1855312709.py:12: DtypeWarning: Columns (20,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traffic = pd.read_csv(csv_file)\n"
     ]
    }
   ],
   "source": [
    "# this code block is for the linked visualization 2-2\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Replace 'your_input.csv' with the actual CSV file path\n",
    "csv_file = 'Traffic_Crashes_-_People.csv'\n",
    "\n",
    "# Replace 'output.json' with the desired JSON file path\n",
    "json_file = 'linked5.json'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "traffic = pd.read_csv(csv_file)\n",
    "traffic_1 = pd.read_csv(\"Traffic_Crashes_-_Crashes_20231011.csv\")\n",
    "selected_weather_conditions = ['BLOWING SNOW', 'CLOUDY/OVERCAST', 'FOG/SMOKE/HAZE', 'FREEZING RAIN/DRIZZLE', 'RAIN', 'SLEET/HAIL', 'SNOW']\n",
    "\n",
    "# Step 1: Filter out 'clear' weather condition\n",
    "filtered_traffic = traffic_1[traffic_1['WEATHER_CONDITION'] != 'CLEAR']\n",
    "\n",
    "# Step 2: Merge the two dataframes on 'CRASH_RECORD_ID'\n",
    "traffic = traffic.merge(filtered_traffic[['CRASH_RECORD_ID', 'WEATHER_CONDITION']], on='CRASH_RECORD_ID', how='inner')\n",
    "traffic = traffic[traffic['WEATHER_CONDITION'].isin(selected_weather_conditions)]\n",
    "\n",
    "# Add the year information to the DataFrame\n",
    "traffic['YEAR'] = pd.to_datetime(traffic['CRASH_DATE']).dt.year\n",
    "\n",
    "# Create a list of dictionaries in the desired format\n",
    "json_data = []\n",
    "\n",
    "# Get unique combinations of WEATHER_CONDITION and YEAR\n",
    "combinations = traffic[['WEATHER_CONDITION', 'YEAR']].drop_duplicates()\n",
    "\n",
    "# Iterate through the combinations and count occurrences\n",
    "for index, row in combinations.iterrows():\n",
    "    weather_condition = row['WEATHER_CONDITION']\n",
    "    year = row['YEAR']\n",
    "    count = len(traffic[(traffic['WEATHER_CONDITION'] == weather_condition) & (traffic['YEAR'] == year)])\n",
    "\n",
    "    data_point = {\n",
    "        'weather_condition': weather_condition,\n",
    "        'count': count,\n",
    "        'year': int(year)  # Include the year component\n",
    "    }\n",
    "    json_data.append(data_point)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(json_file, 'w') as json_output:\n",
    "    json.dump(json_data, json_output, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
